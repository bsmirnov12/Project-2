{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UT-TOR-DATA-PT-01-2020-U-C Group Project 2\n",
    "# Find as many artist wiki pages as possible\n",
    "# Use Artist table for list of artists, save detected Wikipedia page links in Wikilinks table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQLite Database related imports\n",
    "import sqlalchemy\n",
    "from sqlalchemy.engine import Engine\n",
    "from sqlalchemy.ext.automap import automap_base\n",
    "from sqlalchemy.orm import Session\n",
    "from sqlalchemy import create_engine, event, inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping related imports\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Column('id', INTEGER(), table=<Artist>, primary_key=True, nullable=False),\n",
       " Column('name', TEXT(), table=<Artist>, nullable=False),\n",
       " Column('is_band', INTEGER(), table=<Artist>),\n",
       " Column('genre', TEXT(), table=<Artist>),\n",
       " Column('image', TEXT(), table=<Artist>),\n",
       " Column('wiki', TEXT(), table=<Artist>),\n",
       " Column('dob', TEXT(), table=<Artist>),\n",
       " Column('origin', TEXT(), table=<Artist>)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn on PRAGMA foreign_keys to enforce foregn key constraints (it is disabled by default in SQLite)\n",
    "@event.listens_for(Engine, \"connect\")\n",
    "def set_sqlite_pragma(dbapi_connection, connection_record):\n",
    "    cursor = dbapi_connection.cursor()\n",
    "    cursor.execute(\"PRAGMA foreign_keys=ON\")\n",
    "    cursor.close()\n",
    "\n",
    "# Create engine to access the database\n",
    "engine = create_engine(\"sqlite:///../data/CanadaTop100v2.sqlite\")\n",
    "\n",
    "# Reflect an existing database into a new model\n",
    "AutomapBase = automap_base()\n",
    "\n",
    "# Reflect the tables\n",
    "AutomapBase.prepare(engine, reflect=True)\n",
    "\n",
    "# Save references to each table\n",
    "Artist = AutomapBase.classes.Artist\n",
    "\n",
    "# Create our session (link) from Python to the DB\n",
    "db_session = Session(engine)\n",
    "\n",
    "# Debug output\n",
    "#AutomapBase.classes.items()\n",
    "#list(inspect(Artist).columns)\n",
    "\n",
    "# Prepare for massive quering of Wikipedia \n",
    "session = requests.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns list of URLs to Wikipedia pages that possibly match our artist\n",
    "# Parameters:\n",
    "#    session - requests session to use for quering Wikipedia API\n",
    "#    name - artist name\n",
    "# Returns a posibly empty list of URLs to Wikipedia pages\n",
    "# Checks URL suffixes for possible match\n",
    "url_tails = re.compile(r\"_\\(musician|singer|band|rapper|DJ\\)$\")\n",
    "api_url = \"https://en.wikipedia.org/w/api.php\"\n",
    "params = {\n",
    "    \"action\": \"opensearch\",\n",
    "    \"namespace\": \"0\",\n",
    "    \"search\": \"\",\n",
    "    \"limit\": \"15\",\n",
    "    \"format\": \"json\"\n",
    "}\n",
    "\n",
    "def get_wiki_urls(session, name):\n",
    "    params['search'] = name\n",
    "    response = session.get(url=api_url, params=params)\n",
    "    data = response.json()\n",
    "    url_list = data[3]\n",
    "    if not len(url_list):\n",
    "        return []\n",
    "\n",
    "    result = [url_list[0]] # first URL should be the most relevant\n",
    "    for url in url_list[1:]:\n",
    "        if url_tails.search(url):\n",
    "            result.append(url)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tries to detect if a page is about a musician or a band\n",
    "# Returns True if keyword match detected\n",
    "keywords = re.compile(\"Musician|Singer|Rapper|Members|Label\", flags=re.IGNORECASE)\n",
    "\n",
    "def is_good(soup):\n",
    "    infobox = soup.find('table', class_=\"vcard\")\n",
    "    if not infobox:\n",
    "        return False\n",
    "\n",
    "    for s in infobox.strings:\n",
    "        if keywords.match(s):\n",
    "            return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the URL list and try to detect if it a page we're looking for\n",
    "# If the page is detected its url and soup object are returned, otherwise return None\n",
    "def get_wiki_page(session, url_lst):\n",
    "    for url in url_lst:\n",
    "        response = session.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        if is_good(soup):\n",
    "            return url, soup\n",
    "\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function updates Artist table with scraped data\n",
    "def update_artist(artist_id, url, data):\n",
    "    db_session.query(Artist).\\\n",
    "        filter_by(id = artist_id).\\\n",
    "        update({\n",
    "            'is_band' : data.get('is_band'),\n",
    "            'genre' : data.get('genre'),\n",
    "            'image' : data.get('image'),\n",
    "            'wiki' : url,\n",
    "            'dob' : data.get('dob'),\n",
    "            'origin' : data.get('origin')\n",
    "        })\n",
    "    db_session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion parses Genre info\n",
    "genre_template = re.compile(r\"[\\w][-&/ \\w]*\")\n",
    "\n",
    "def parse_genre(td):\n",
    "    for s in td.strings:\n",
    "        g = genre_template.match(s)\n",
    "        if g: return g[0]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion parses Origin info\n",
    "origin_template = re.compile(r\"[\\w, ]\")\n",
    "\n",
    "def parse_origin(td):\n",
    "    origin = \"\"\n",
    "    for s in td.strings:\n",
    "        if origin_template.match(s):\n",
    "            origin += s\n",
    "    if len(origin):\n",
    "        return origin\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion parses Birth info (date and place)\n",
    "def parse_born(td):\n",
    "    # Handle birth date\n",
    "    bday = None\n",
    "    span = td.find('span', class_='bday')\n",
    "    if span:\n",
    "        bday = str(span.string)\n",
    "\n",
    "    # Handle Birth place\n",
    "    bplace = \"\"\n",
    "    td_brs = td.find_all('br') # May be 2 <br> - 1st after birth name,\n",
    "    if td_brs:                 # 2nd after birth date. Birth name may be missing, so only 1 <br> is possible\n",
    "        td_br = td_brs[-1]     # We look for the last <br>\n",
    "    else:\n",
    "        td_br = None\n",
    "        \n",
    "    if td_br and td_br.next_siblings:\n",
    "        for sib in td_br.next_siblings:\n",
    "            try:\n",
    "                for s in sib.strings:\n",
    "                    if origin_template.match(s):\n",
    "                        bplace += s\n",
    "            except:\n",
    "                if origin_template.match(sib):\n",
    "                    bplace += sib\n",
    "\n",
    "    if len(bplace) == 0:\n",
    "        bplace = None # We need NULL in the table, and \"\" isn't NULL, only None is NULL\n",
    "    \n",
    "    return bday, bplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on 'scrape_wikipedia' by Leah Lindy\n",
    "# Refactored to accomodate new interface and improve search of data\n",
    "header_genre  = re.compile(\"Genres?$\", re.I)\n",
    "header_born   = re.compile(\"Born$\", re.I)\n",
    "header_origin = re.compile(\"Origin$\", re.I)\n",
    "header_member = re.compile(\"Members?$\", re.I)\n",
    "\n",
    "def soup_to_data(soup): \n",
    "    # Initialize columns\n",
    "    is_band = None\n",
    "    genre = None\n",
    "    image = None\n",
    "    dob = None\n",
    "    origin = None\n",
    "    pob = None # Sometimes they give place of birth and origin separately\n",
    "    \n",
    "    # Step 1: Get Info Table\n",
    "    artist_table = soup.find('table', class_='infobox')\n",
    "\n",
    "    # Step 2: Find image\n",
    "    table_img = artist_table.find('img')\n",
    "    if table_img:\n",
    "        image = 'https:' + table_img['src']\n",
    "\n",
    "    # Steps 3 and so on: Iterate through table rows and try to squeeze useful data from them\n",
    "    headers = artist_table.find_all('th')\n",
    "    for th in headers:\n",
    "        if header_genre.match(str(th.string)):\n",
    "            genre = parse_genre(th.next_sibling)\n",
    "            if genre: # it is a musician, but we don't know if it's a person or band.\n",
    "                is_band = 0 # 'Members' section goes after 'Genres'. If it is there, band it is\n",
    "        elif header_born.match(str(th.string)):\n",
    "            dob, pob = parse_born(th.next_sibling)\n",
    "            if dob: is_band = 0 # Sometimes DOB isn't recognized, e.g. https://en.wikipedia.org/wiki/Saint_Jhn\n",
    "        elif header_origin.match(str(th.string)):\n",
    "            origin = parse_origin(th.next_sibling)\n",
    "        elif header_member.match(str(th.string)):\n",
    "            is_band = 1\n",
    "\n",
    "    # Handle situation when they give place of birth but don't give Origin\n",
    "    # If they give both prefer Origin over place of birth\n",
    "    if origin is None:\n",
    "        origin = pob\n",
    "\n",
    "    return {\n",
    "        'is_band': is_band,\n",
    "        'genre': genre,\n",
    "        'image': image,\n",
    "        'dob': dob,\n",
    "        'origin': origin\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrapes wiki page and saves scraped data into Artist table\n",
    "# Parameters:\n",
    "#    artist_id - artist/band id in the Artist table\n",
    "#    url - link to Wikipedia page to save in the Artist table\n",
    "#    soup - page contents to scrape\n",
    "# Returns True if success, false otherwise\n",
    "def scrape_wiki(artist_id, url, soup):\n",
    "    data = soup_to_data(soup)\n",
    "    if not data:\n",
    "        return False\n",
    "    else:\n",
    "        update_artist(artist_id, url, data)\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Page Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all artists that haven't been scraped yet\n",
    "artists = db_session.query(Artist.id, Artist.name).filter_by(wiki=None).all()\n",
    "\n",
    "# Prepare for massive quering of Wikipedia \n",
    "session = requests.Session()\n",
    "\n",
    "# Main loop \n",
    "for artist_res in artists:\n",
    "    artist_dct = artist_res._asdict()\n",
    "    artist_id = artist_dct['id']\n",
    "    name = artist_dct['name']\n",
    "    print(artist_id, end=\"\")\n",
    "\n",
    "    # Get list of URLs to Wikipedia pages\n",
    "    url_lst = get_wiki_urls(session, name)\n",
    "    if len(url_lst) == 0:\n",
    "        print(\"- \", end=\"\")\n",
    "        continue\n",
    "\n",
    "    # Find a page that looks like what we need\n",
    "    url, soup = get_wiki_page(session, url_lst)\n",
    "    if soup is None:\n",
    "        print(\"- \", end=\"\")\n",
    "        continue\n",
    "\n",
    "    # Scrape the page and update DB\n",
    "    res = scrape_wiki(artist_id, url, soup)\n",
    "    if res:\n",
    "        print(\"+ \", end=\"\")\n",
    "    else:\n",
    "        print(\"- \", end=\"\")\n",
    "\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from manually added Wikipedia links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artists = db_session.query(Artist.id, Artist.wiki).filter_by(is_band = None).filter(Artist.wiki != None).all()\n",
    "\n",
    "# Main loop \n",
    "for artist_res in artists:\n",
    "    artist_dct = artist_res._asdict()\n",
    "    artist_id = artist_dct['id']\n",
    "    url = artist_dct['wiki']\n",
    "    print(artist_id, end=\"\")\n",
    "\n",
    "    # Find a page that looks like what we need\n",
    "    url, soup = get_wiki_page(session, [url])\n",
    "    if soup is None:\n",
    "        print(\"- \", end=\"\")\n",
    "        continue\n",
    "\n",
    "    # Scrape the page and update DB\n",
    "    res = scrape_wiki(artist_id, url, soup)\n",
    "    if res:\n",
    "        print(\"+ \", end=\"\")\n",
    "    else:\n",
    "        print(\"- \", end=\"\")\n",
    "\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "session = requests.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/Tones_and_I'\n",
    "response = session.get(url)\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "artist_table=soup.find('table', class_='infobox')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOB: None  POB: None\n",
      "Origin: Frankston, Victoria, Australia\n",
      "Genre: Pop\n",
      "is_band=0\n"
     ]
    }
   ],
   "source": [
    "headers = artist_table.find_all('th')\n",
    "td = None\n",
    "is_band = None\n",
    "for th in headers:\n",
    "    if re.match(\"Genre\", str(th.string), re.I):\n",
    "        genre = parse_genre(th.next_sibling)\n",
    "        print(\"Genre:\", genre)\n",
    "        is_band = 0\n",
    "    elif re.match(\"Born\", str(th.string), re.I):\n",
    "        dob, pob = parse_born(th.next_sibling)\n",
    "        print(\"DOB:\", dob, \" POB:\", pob)\n",
    "        if dob: is_band = 0\n",
    "    elif re.match(\"Origin\", str(th.string), re.I):\n",
    "        origin = parse_origin(th.next_sibling)\n",
    "        print(\"Origin:\", origin)\n",
    "    elif re.match(\"Member\", str(th.string), re.I):\n",
    "        print(\"is band\")\n",
    "        is_band = 1\n",
    "print(f\"is_band={is_band}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-PythonData] *",
   "language": "python",
   "name": "conda-env-.conda-PythonData-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
